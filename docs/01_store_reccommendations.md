# Engineering Plan: Rich Media Character Generator on Fly.io

This document provides a complete engineering plan to build and deploy a web-based character generator. The application will allow users to select from a set of predefined categories and adjust numerical sliders to find and display 3D avatars that match their criteria. This plan has been updated to incorporate rich media assets (Lottie JSON or Godot scenes) as the primary visual representation of characters, alongside Markdown for descriptions.

This plan assumes the use of the custom data model (`c9a4b1d0-a3e1-4b7e-8a0f-5b1d7e3c2b9f`) and leverages the `uro` Elixir/Phoenix application as its backend, to be deployed on the Fly.io platform.

## 1. Project Overview and Architecture

The final application will consist of three main components, following a more integrated architecture thanks to Phoenix LiveView:

### Frontend (Phoenix LiveView)

The user interface will be built using Phoenix LiveView, Elixir's native tooling for creating rich, real-time user experiences. This eliminates the need for a separate JavaScript framework, as the frontend will be rendered on the server and updated dynamically over a persistent WebSocket connection. The LiveView will be responsible for handling user interactions and orchestrating the rendering of Lottie or Godot scene files via client-side hooks.

### Application Logic (Uro - Phoenix)

The Elixir/Phoenix application serves as the core of the system. It will handle user events from the LiveView, query the database based on user criteria, and manage the application state.

### Database (PostgreSQL)

The database containing all the avatar data ingested from marketplaces, the manual labels (tags), numerical attributes, and links to the associated Lottie/Godot scene files.

This architecture is perfectly suited for deployment on Fly.io, which excels at running full-stack Elixir applications and their associated databases.

## 2. Phase 0: Data Acquisition and Ingestion

This initial phase focuses on populating the database with item data from external marketplaces.

### 2.1. Objective

To programmatically scrape item data (e.g., Avatars, 3D Fashion) and their preview images from marketplaces like booth.pm.

### 2.2. Implementation

- **Develop a Scraper Script**: Use a tool like the BoothPM-SDK to search for and retrieve information on items.
- **Acquire Data**: For each item, extract its metadata (ID, name, description) and, critically, download its primary preview image.
- **Store Data**: Save the metadata and the path to the downloaded image into a structured intermediate file, such as `scraped_data.json`.

## 3. Phase 1: Automated Proportional Analysis via Keypoint Estimation

This new phase uses computer vision to analyze the scraped images and automatically tag avatars based on their body proportions, using your anthropometric research as a ground truth.

### 3.1. Objective

To programmatically extract skeletal keypoints from avatar images, calculate their bone proportions, and tag them based on a comparison with realistic human data.

### 3.2. Implementation: Teacher-Student Model Training

To achieve the highest accuracy with the most efficient workflow, we will use a teacher-student model approach. This combines the broad capabilities of a pre-trained model (the "teacher") with the specialized accuracy of a custom model (the "student").

#### Auto-Labeling with the Teacher Model (MediaPipe)

1. First, run the entire dataset of scraped avatar images through Google's MediaPipe Pose model.
2. For each image, MediaPipe will automatically predict the locations of its standard human pose landmarks.
3. A script will map these MediaPipe landmarks to the required VRM joints (e.g., mapping `left_shoulder` to `leftUpperArm`).
4. This process generates a large, foundational dataset of keypoint labels automatically, saving hundreds of hours of manual work.

#### Label Refinement and Curation (Roboflow)

1. Upload the images and their auto-generated MediaPipe labels into a new project on Roboflow.
2. The task for the human annotator is now much simpler: instead of labeling from scratch, you will review and correct the labels generated by MediaPipe.
3. This "human-in-the-loop" step is crucial for fixing errors where the general-purpose MediaPipe model struggles, particularly on stylized or anime-like avatars.

#### Training the Student Model (Roboflow)

1. Once the dataset has been corrected and refined, use Roboflow's platform to train a new, custom keypoint detection model.
2. This "student" model is trained specifically on your curated dataset of VR avatars. It will learn the specific visual patterns of your data and will be significantly more accurate for your use case than the original MediaPipe "teacher" model was.

#### Create an Analysis Script

This script will iterate through the `scraped_data.json` file from the previous phase. For each avatar:

1. **Run Inference**: Load the avatar's preview image and run the newly trained student model on it. The output will be a highly accurate list of `(x, y)` coordinates for each VRM joint.
2. **Calculate Bone Lengths**: Calculate the pixel distance between connected joints (e.g., the distance between the `leftShoulder` and `leftElbow` keypoints).
3. **Normalize Proportions**: This is the most critical step. To make comparisons meaningful, all bone lengths must be normalized. A robust method is to divide each bone's pixel length by the avatar's total pixel height (e.g., the vertical distance from the `head` keypoint to the average position of the `feet` keypoints). This gives you a set of proportional ratios (e.g., `upperArm_ratio = upperArm_pixels / total_height_pixels`).
4. **Compare and Tag**: Compare the calculated proportional ratios against the "ground truth" ratios derived from your anthropometric research. For example, if your research shows the median male `upperLeg` to stature ratio is `0.235`, and the analyzed avatar has a ratio of `0.240`, you can tag it as `"proportions:realistic_male"`. If the ratio is `0.350`, you could tag it as `"proportions:stylized_long_legs"`.
5. **Store Results**: Add these generated tags and the calculated numeric proportions to the avatar's record in your database.

## 4. Phase 2: Building the Application Logic

This phase focuses on creating the core query functionality within the Phoenix application's context layer.

### 4.1. Objective

Create a reusable function within the Marketplace context that can query the database for avatars based on a dynamic set of tags and numerical slider ranges.

### 4.2. Implementation

The query logic will be encapsulated in a context function. This function will be called directly from the LiveView process, eliminating the need for a separate controller or API endpoint for the main UI.

```elixir
# lib/uro/marketplace/marketplace.ex (new function)
def find_avatars_by_criteria(tags \\ [], sliders \\ %{}) do
  # Start with a base query for 3D Characters
  query = from(c in Uro.UserContent.ThreeDCharacter)

  # Dynamically build the query for tags
  query =
    Enum.reduce(tags, query, fn tag_name, acc_query ->
      from q in acc_query,
        join: t in assoc(q, :tags),
        where: t.name == ^tag_name
    end)

  # Dynamically build the query for sliders
  query =
    Enum.reduce(sliders, query, fn {slider_name, range}, acc_query ->
      min_val = Map.get(range, "min")
      max_val = Map.get(range, "max")

      # This uses PostgreSQL's JSONB query capabilities
      from q in acc_query,
        where: fragment("(?->>?)::float >= ?", q.numeric_attributes, ^slider_name, ^min_val),
        where: fragment("(?->>?)::float <= ?", q.numeric_attributes, ^slider_name, ^max_val)
    end)

  # Preload associations and fetch the results
  query
  |> preload(:tags, :shop)
  |> Repo.all()
end
```

## 5. Phase 3: Building the User65nwaq Interface with Phoenix LiveView

This phase focuses on creating the interactive user interface using Elixir-native tools.

### 5.1. Objective

Develop a real-time, interactive user interface using Phoenix LiveView that allows users to select criteria and renders the results, including the rich media assets, without page reloads.

### 5.2. Implementation

- **Create a LiveView**: Generate a new LiveView to manage the character generator page (e.g., `CharacterGeneratorLive`).
- **State Management**: The state of selected tags and slider values will be stored in the LiveView process's socket assigns.
- **Event Handling**: User interactions will be handled using LiveView bindings:
  - `phx-click` on category buttons will send an event to the LiveView process to add or remove a tag from the filter list.
  - `phx-change` and `phx-debounce` on the slider inputs will send an event with the new numerical range.
- **Data Fetching**: Inside the `handle_event` callbacks in your LiveView, you will call the `Marketplace.find_avatars_by_criteria/2` function directly. The results will be stored in the assigns, which will automatically trigger a re-render of the page.

#### Rendering Rich Media with LiveView Hooks

- **Markdown**: Use a library like Earmark to parse the `markdown_description` string into HTML within your HEEx template.
- **Lottie/Godot**: Use LiveView Hooks. A small amount of client-side JavaScript will be needed.
  - In your HEEx template, the element that will contain the animation will have a `phx-hook` attribute (e.g., `<div phx-hook="LottiePlayer" data-asset-url={@avatar.asset_url}></div>`).
  - You will write a JavaScript hook object that defines `mounted` and `updated` callbacks. These callbacks will read the `data-asset-url` from the element and use a library like `lottie-web` to load and render the animation, or set the `src` of an `<iframe>` for a Godot scene.

## 6. Phase 4: Deployment to Fly.io

The deployment process is now more straightforward as we are deploying a single, cohesive Phoenix application.

### 6.1. Objective

Package and deploy the Phoenix application, its database, and a storage solution for the Lottie/Godot files.

### 6.2. Implementation Steps

1. **Install flyctl & Generate Dockerfile**: (No change from previous plan)
2. **Launch the App & Database on Fly.io**: (No change from previous plan)
3. **Set Up Storage for Assets**: Use Fly Volumes or a cloud storage provider like S3. (No change from previous plan)
4. **Configure Secrets & Deploy**: (No change from previous plan)
5. **Run Database Migrations**: (No change from previous plan)

This updated plan fully integrates your vision for a rich media character generator, creating a powerful and flexible application architecture ready for deployment.
